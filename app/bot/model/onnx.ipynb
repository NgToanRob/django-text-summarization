{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimum API for Deployment with ONNX and inference\n",
        "Tutorial in the [link](https://www.philschmid.de/optimizing-transformers-with-optimum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plhg5gS-Tfit"
      },
      "source": [
        "## 1. Install `Optimum` for Onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b1el_LmQf1W",
        "outputId": "e56d4680-46ba-4b0b-b5a0-4cb6ec840dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optimum[onnxruntime] in /usr/local/lib/python3.10/dist-packages (1.8.6)\n",
            "Requirement already satisfied: evaluate[evaluator] in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (15.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (1.11.1)\n",
            "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (4.29.2)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (0.15.2+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (0.14.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (2.12.0)\n",
            "Requirement already satisfied: onnx<1.14.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (1.12.0)\n",
            "Requirement already satisfied: onnxruntime>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (3.20.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate[evaluator]) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate[evaluator]) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate[evaluator]) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate[evaluator]) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate[evaluator]) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate[evaluator]) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate[evaluator]) (2023.4.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate[evaluator]) (0.18.0)\n",
            "Requirement already satisfied: scipy>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from evaluate[evaluator]) (1.10.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime]) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime]) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime]) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (4.5.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.9.0->optimum[onnxruntime]) (23.3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate[evaluator]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate[evaluator]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate[evaluator]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate[evaluator]) (3.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[onnxruntime]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[onnxruntime]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[onnxruntime]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->optimum[onnxruntime]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->optimum[onnxruntime]) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (0.13.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (0.1.99)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum[onnxruntime]) (10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate[evaluator]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate[evaluator]) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum[onnxruntime]) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->optimum[onnxruntime]) (8.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate[evaluator]) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum[onnxruntime]) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"optimum[onnxruntime]\" evaluate[evaluator] --upgrade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf4jSk5ETkQO"
      },
      "source": [
        "## 2. Convert a Hugging Face Transformers model to ONNX for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-24 23:36:26.986719: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-05-24 23:36:27.070553: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-05-24 23:36:27.071454: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-24 23:36:28.334248: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
        "\n",
        "model_id = \"VietAI/vit5-base-vietnews-summarization\"\n",
        "onnx_path = Path(\"onnx\")\n",
        "task = \"summarization\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tZ3biBLQvub",
        "outputId": "1d6eaed2-52b3-4966-c57e-0ff653cbbcd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Framework not specified. Using pt to export to ONNX.\n",
            "Using framework PyTorch: 2.0.1+cu118\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using framework PyTorch: 2.0.1+cu118\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:832: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if causal_mask.shape[1] < attention_mask.shape[1]:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using framework PyTorch: 2.0.1+cu118\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "Asked a sequence length of 16, but a sequence length of 1 will be used with use_past == True for `decoder_input_ids`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:507: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  elif past_key_value.shape[2] != key_value_states.shape[1]:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# load vanilla transformers and convert to onnx\n",
        "model = ORTModelForSeq2SeqLM.from_pretrained(onnx_path, export=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(onnx_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwI2sF8QQzo9",
        "outputId": "8b6c6769-7760-4757-87bb-818f752568ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('onnx/tokenizer_config.json',\n",
              " 'onnx/special_tokens_map.json',\n",
              " 'onnx/spiece.model',\n",
              " 'onnx/added_tokens.json',\n",
              " 'onnx/tokenizer.json')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# save onnx checkpoint and tokenizer\n",
        "model.save_pretrained(onnx_path)\n",
        "tokenizer.save_pretrained(onnx_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WofrGfS3Rpwu"
      },
      "outputs": [],
      "source": [
        "# test the model with using transformers pipeline, with handle_impossible_answer for squad_v2\n",
        "optimum_summarizer = pipeline(task, model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIIe0gzsSTkg",
        "outputId": "1c383a81-6baf-42b1-c8a9-a933c5b4fd27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'summary_text': 'ViệtAI là tổ chức phi lợi nhuận duy nhất được thành lập năm 2003'}]\n"
          ]
        }
      ],
      "source": [
        "text = 'VietAI là tổ chức phi lợi nhuận với sứ mệnh ươm mầm tài năng về trí tuệ nhân tạo và xây dựng một cộng đồng các chuyên gia trong lĩnh vực trí tuệ nhân tạo đẳng cấp quốc tế tại Việt Nam.'\n",
        "prediction = optimum_summarizer(text)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61QUW-_RTpzM"
      },
      "source": [
        "## 3. Use the ORTOptimizer to optimize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9vZ6lP7TbMr",
        "outputId": "e4ca82c5-1947-451b-9061-910b5b626f8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/optimum/onnxruntime/configuration.py:735: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
            "  warnings.warn(\n",
            "Optimizing model...\n",
            "Configuration saved in onnx/ort_config.json\n",
            "Optimized model saved at: onnx (external data format: False; saved all tensor to one file: True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('onnx')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from optimum.onnxruntime import ORTOptimizer\n",
        "from optimum.onnxruntime.configuration import OptimizationConfig\n",
        "\n",
        "# create ORTOptimizer and define optimization configuration\n",
        "optimizer = ORTOptimizer.from_pretrained(model)\n",
        "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
        "\n",
        "# apply the optimization configuration to the model\n",
        "optimizer.optimize(\n",
        "    save_dir=onnx_path,\n",
        "    optimization_config=optimization_config,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0R8x71Sqxc5",
        "outputId": "6115ac6f-05f3-4324-ca3b-befc41691622"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'summary_text': 'ViệtAI là tổ chức phi lợi nhuận duy nhất được thành lập năm 2003'}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# load optimized model\n",
        "model = ORTModelForSeq2SeqLM.from_pretrained(onnx_path, file_name=\"model_optimized.onnx\")\n",
        "\n",
        "# create optimized pipeline\n",
        "optimized_summarizer = pipeline(task, model=model, tokenizer=tokenizer)\n",
        "optimized_summarizer(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVujYqZljtWE"
      },
      "source": [
        "## 4. Apply dynamic quantization using ORTQuantizer from Optimum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UspAoxWpjwHw",
        "outputId": "50fcc168-88d8-4d67-dd12-111d9bbeb89e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
            "Quantizing model...\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.0/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.0/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.1/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.1/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.2/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.2/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.3/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.3/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.4/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.4/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.5/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.5/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.6/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.6/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.7/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.7/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.8/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.8/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.9/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.9/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.10/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.10/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.11/layer.0/SelfAttention/Transpose_3_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /decoder/block.11/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "Saving quantized model at: onnx (external data format: False)\n",
            "Configuration saved in onnx/ort_config.json\n",
            "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
            "Quantizing model...\n",
            "Saving quantized model at: onnx (external data format: False)\n",
            "Configuration saved in onnx/ort_config.json\n",
            "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
            "Quantizing model...\n",
            "WARNING:root:Failed to infer data type of tensor: /block.0/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.0/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.0/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.0/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.1/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.1/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.1/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.1/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.2/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.2/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.2/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.2/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.3/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.3/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.3/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.3/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.4/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.4/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.4/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.4/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.5/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.5/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.5/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.5/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.6/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.6/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.6/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.6/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.7/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.7/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.7/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.7/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.8/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.8/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.8/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.8/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.9/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.9/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.9/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.9/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.10/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.10/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.10/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.10/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.11/layer.0/SelfAttention/Transpose_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.11/layer.0/SelfAttention/Transpose_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.11/layer.0/SelfAttention/Transpose_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "WARNING:root:Failed to infer data type of tensor: /block.11/layer.0/SelfAttention/Softmax_output_0. Please add data type info for this tensor if your model has customized operators.\n",
            "Saving quantized model at: onnx (external data format: False)\n",
            "Configuration saved in onnx/ort_config.json\n"
          ]
        }
      ],
      "source": [
        "from optimum.onnxruntime import ORTQuantizer\n",
        "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
        "\n",
        "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
        "\n",
        "# create ORTQuantizer and define quantization configuration\n",
        "optimized_filenames = [\n",
        "    'decoder_model_optimized.onnx',\n",
        "    'decoder_with_past_model_optimized.onnx',\n",
        "    'encoder_model_optimized.onnx', \n",
        "]\n",
        "for filename in optimized_filenames:\n",
        "  dynamic_quantizer = ORTQuantizer.from_pretrained(model_or_path=onnx_path,\n",
        "                                                   file_name=filename)\n",
        "\n",
        "  # apply the quantization configuration to the model\n",
        "  model_quantized_path = dynamic_quantizer.quantize(\n",
        "      save_dir=onnx_path,\n",
        "      quantization_config=dqconfig,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdGCXgrh2l2Z",
        "outputId": "313bf562-7d69-49fc-9cef-32a1681598e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/pre-entrance\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/pre-entrance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e7OKvFC2qj9",
        "outputId": "a3ff22bc-d9f2-45e5-d367-64e843a76ec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model file size: 643.82 MB\n",
            "Quantized Model file size: 240.73 MB\n",
            "Model file size: 589.80 MB\n",
            "Quantized Model file size: 227.21 MB\n",
            "Model file size: 429.92 MB\n",
            "Quantized Model file size: 187.01 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# get model file size\n",
        "quantized_filenames = [\n",
        "    'decoder_model_optimized_quantized.onnx',\n",
        "    'decoder_with_past_model_optimized_quantized.onnx',\n",
        "    'encoder_model_optimized_quantized.onnx', \n",
        "]\n",
        "for i in range(len(quantized_filenames)):\n",
        "  size = os.path.getsize(onnx_path / optimized_filenames[i])/(1024*1024)\n",
        "  quantized_model = os.path.getsize(onnx_path / quantized_filenames[i])/(1024*1024)\n",
        "\n",
        "  print(f\"Model file size: {size:.2f} MB\")\n",
        "  print(f\"Quantized Model file size: {quantized_model:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMLA_evb_ZqZ"
      },
      "source": [
        "## 5. Test inference with the quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys7VVEvs_dWl",
        "outputId": "42c7f361-ff23-4547-845c-a6bf1584edf4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The ONNX file decoder_model_optimized_quantized.onnx is not a regular name used in optimum.onnxruntime that are ['decoder_model.onnx', 'decoder_model_quantized.onnx', 'decoder_model_optimized.onnx'], the ORTModelForSeq2SeqLM might not behave as expected.\n",
            "The ONNX file decoder_with_past_model_optimized_quantized.onnx is not a regular name used in optimum.onnxruntime that are ['decoder_with_past_model.onnx', 'decoder_with_past_model_quantized.onnx', 'decoder_with_past_model_optimized.onnx'], the ORTModelForSeq2SeqLM might not behave as expected.\n",
            "The ONNX file encoder_model_optimized_quantized.onnx is not a regular name used in optimum.onnxruntime, the ORTModelForConditionalGeneration might not behave as expected.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# load optimized model\n",
        "model = ORTModelForSeq2SeqLM.from_pretrained(onnx_path, file_name=\"model_quantized.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(onnx_path)\n",
        "\n",
        "# create optimized pipeline\n",
        "optimized_summarizer = pipeline(task, model=model, tokenizer=tokenizer)\n",
        "\n",
        "text = 'VietAI là tổ chức phi lợi nhuận với sứ mệnh ươm mầm tài năng về trí tuệ nhân tạo và xây dựng một cộng đồng các chuyên gia trong lĩnh vực trí tuệ nhân tạo đẳng cấp quốc tế tại Việt Nam.'\n",
        "\n",
        "output = optimized_summarizer(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ViệtAI là tổ chức phi lợi nhuận duy nhất trên thế giới về trí tuệ nhân tạo\n"
          ]
        }
      ],
      "source": [
        "print(output[0]['summary_text'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
